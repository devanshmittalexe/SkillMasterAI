# ğŸ“ SkillMirror AI
### An Autonomous Self-Evaluation Agent powered by Claude

SkillMirror is an agentic AI system that evaluates your knowledge on any subject. You pick the topic, Claude drives everything â€” breaking it into subtopics, generating questions, collecting answers, grading them, and tracking your progress over time.

No hardcoded pipelines. Claude reasons and decides every step.

---

## ğŸ§  How It Works

```
You choose a subject
        â†“
Claude analyzes topics
        â†“
Claude generates questions
        â†“
You answer one by one
        â†“
Claude grades every answer
        â†“
Results saved + history compared
        â†“
Full report with scores & feedback
```

Claude is the brain. The tools are just messengers.

---

## ğŸ“ Project Structure

```
skill_mirror/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ settings.py           # Model, tokens, system prompt
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ topic_tools.py        # analyze_topics, generate_questions
â”‚   â”œâ”€â”€ question_tools.py     # ask_user_question
â”‚   â”œâ”€â”€ grading_tools.py      # grade_answer
â”‚   â”œâ”€â”€ history_tools.py      # fetch_history, save_evaluation_result
â”‚   â””â”€â”€ flag_tools.py         # flag_incomplete_evaluation
â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ tool_definitions.py   # Claude's tool manual (schemas)
â”‚   â”œâ”€â”€ tool_executor.py      # Routes tool calls to functions
â”‚   â””â”€â”€ loop.py               # The agentic loop
â”œâ”€â”€ data/
â”‚   â””â”€â”€ evaluation_history.json  # Persistent history stored here
â”œâ”€â”€ main.py                   # Entry point
â”œâ”€â”€ __init__.py
â”œâ”€â”€ .env                      # Your API key (never commit this)
â”œâ”€â”€ .gitignore
â””â”€â”€ requirements.txt
```

---

## âš™ï¸ Setup

### 1. Clone the repo
```bash
git clone https://github.com/devanshmittalexe/SkillMasterAI.git
cd SkillMasterAI/skill_mirror
```

### 2. Create a virtual environment
```bash
python -m venv venv

# Windows
venv\Scripts\activate

# Mac/Linux
source venv/bin/activate
```

### 3. Install dependencies
```bash
pip install -r requirements.txt
```

### 4. Add your Anthropic API key
Create a `.env` file inside `skill_mirror/`:
```
ANTHROPIC_API_KEY=your_api_key_here
```
Get your key at [console.anthropic.com](https://console.anthropic.com)

### 5. Run it
```bash
python main.py
```

---

## ğŸš€ Example Session

```
======================================================================
                       ğŸ“ SELF EVALUATION AGENT
======================================================================
Options:
  1. Start evaluation
  2. Exit

Select (1-2): 1
What subject should I evaluate you on? Chernobyl Disaster
How many questions? (default 5): 3

  ğŸ”§ Agent calling: analyze_topics
  ğŸ”§ Agent calling: generate_questions
  ğŸ”§ Agent calling: ask_user_question

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Question 1 | Topic: Causes of the Disaster | Difficulty: MEDIUM
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  What design flaw in the RBMK-1000 reactor contributed to the explosion?

  Your answer: The positive void coefficient made it unstable at low power

  ğŸ”§ Agent calling: grade_answer
  ğŸ”§ Agent calling: fetch_history
  ğŸ”§ Agent calling: save_evaluation_result

ğŸ“¢ AGENT REPORT
  Score: 9/10 â€” Excellent answer!
  Overall: 78% | Strong: Reactor Design | Weak: Timeline of Events
```

---

## ğŸ› ï¸ Tech Stack

| Tool | Purpose |
|------|---------|
| Python 3.11+ | Core language |
| Anthropic SDK | Claude API access |
| python-dotenv | Environment variable management |
| JSON | Persistent history storage |
| Git | Version control |

---

## ğŸ”‘ Key Concepts Learned

Building this project teaches:

- **Agentic AI** â€” how LLMs drive multi-step workflows using tools
- **Tool Use** â€” giving Claude a manual and letting it decide when to call what
- **Version Control** â€” proper Git branching with `dev â†’ QA â†’ main`
- **Project Structure** â€” separating concerns across files and modules
- **Prompt Engineering** â€” writing system prompts that enforce strict workflows

---

## ğŸ—ºï¸ Roadmap

- [x] Agentic evaluation loop
- [x] Persistent history tracking
- [x] Multi-topic question generation
- [ ] Adaptive difficulty (harder questions if scoring well)
- [ ] Per-question feedback during quiz
- [ ] ASCII progress charts
- [ ] Web UI

---

## âš ï¸ Important

- Never commit your `.env` file â€” it contains your API key
- Each evaluation session costs approximately $0.05â€“$0.20 depending on question count
- History is stored locally in `data/evaluation_history.json`

---

## ğŸ“„ License

MIT License â€” feel free to use, modify, and build on this project.

---

*Built step by step as a learning project for Agentic AI + Version Control*